{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Convolutional Neural Networks to Predict Emotions Through Visual Cues in Facial Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_load import convert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\n",
      "First few rows of the CSV:\n",
      "  user.id                            image   emotion\n",
      "0     628  facial-expressions_2868588k.jpg     anger\n",
      "1     628  facial-expressions_2868585k.jpg  surprise\n",
      "2     628  facial-expressions_2868584k.jpg   disgust\n",
      "3     628  facial-expressions_2868582k.jpg      fear\n",
      "4   dwdii           Aaron_Eckhart_0001.jpg   neutral\n",
      "\n",
      "Sample filepaths in DataFrame:\n",
      "  Image: facial-expressions_2868588k.jpg\n",
      "  Filepath: c:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\images\\facial-expressions_2868588k.jpg\n",
      "  Image: facial-expressions_2868585k.jpg\n",
      "  Filepath: c:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\images\\facial-expressions_2868585k.jpg\n",
      "  Image: facial-expressions_2868584k.jpg\n",
      "  Filepath: c:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\images\\facial-expressions_2868584k.jpg\n",
      "\n",
      "Encoded emotion classes: ['ANGER' 'CONTEMPT' 'DISGUST' 'FEAR' 'HAPPINESS' 'NEUTRAL' 'SADNESS'\n",
      " 'SURPRISE']\n",
      "\n",
      "Processing image 1/13690: facial-expressions_2868588k.jpg\n",
      "  Original image mode: RGB, size: (347, 536)\n",
      "  Skipping image with incorrect shape: (536, 347)\n",
      "\n",
      "Processing image 2/13690: facial-expressions_2868585k.jpg\n",
      "  Original image mode: RGB, size: (355, 536)\n",
      "  Skipping image with incorrect shape: (536, 355)\n",
      "\n",
      "Processing image 3/13690: facial-expressions_2868584k.jpg\n",
      "  Original image mode: RGB, size: (355, 536)\n",
      "  Skipping image with incorrect shape: (536, 355)\n",
      "\n",
      "Processing image 101/13690: Adrien_Brody_0004.jpg\n",
      "\n",
      "Processing image 201/13690: Albert_Pujols_0001.jpg\n",
      "\n",
      "Processing image 301/13690: Alex_Holmes_0001.jpg\n",
      "\n",
      "Processing image 401/13690: Alvaro_Uribe_0026.jpg\n",
      "\n",
      "Processing image 501/13690: Ana_Guevara_0004.jpg\n",
      "\n",
      "Processing image 601/13690: Andy_Lau_0001.jpg\n",
      "\n",
      "Processing image 701/13690: Annie_Machon_0001.jpg\n",
      "\n",
      "Processing image 801/13690: Ariel_Sharon_0031.jpg\n",
      "\n",
      "Processing image 901/13690: Arnold_Schwarzenegger_0018.jpg\n",
      "\n",
      "Processing image 1001/13690: Augustin_Calleri_0003.jpg\n",
      "\n",
      "Processing image 1101/13690: Ben_Howland_0001.jpg\n",
      "\n",
      "Processing image 1201/13690: Bill_Clinton_0019.jpg\n",
      "\n",
      "Processing image 1301/13690: Bill_Sizemore_0001.jpg\n",
      "\n",
      "Processing image 1401/13690: Boris_Berezovsky_0002.jpg\n",
      "\n",
      "Processing image 1501/13690: Britney_Spears_0006.jpg\n",
      "\n",
      "Processing image 1601/13690: Carla_Sullivan_0001.jpg\n",
      "\n",
      "Processing image 1701/13690: Carolina_Kluft_0003.jpg\n",
      "\n",
      "Processing image 1801/13690: Chanda_Rubin_0003.jpg\n",
      "\n",
      "Processing image 1901/13690: Cheryl_Little_0001.jpg\n",
      "\n",
      "Processing image 2001/13690: Chris_Matthews_0001.jpg\n",
      "\n",
      "Processing image 2101/13690: Colin_Cowie_0001.jpg\n",
      "\n",
      "Processing image 2201/13690: Colin_Powell_0086.jpg\n",
      "\n",
      "Processing image 2301/13690: Colin_Powell_0187.jpg\n",
      "\n",
      "Processing image 2401/13690: Courtney_Cox_0001.jpg\n",
      "\n",
      "Processing image 2501/13690: Dan_Duquette_0001.jpg\n",
      "\n",
      "Processing image 2601/13690: David_Brinkley_0001.jpg\n",
      "\n",
      "Processing image 2701/13690: Dawn_Staley_0001.jpg\n",
      "\n",
      "Processing image 2801/13690: Diana_Renee_Valdivieso_Dubon_0001.jpg\n",
      "\n",
      "Processing image 2901/13690: Donald_Rumsfeld_0004.jpg\n",
      "\n",
      "Processing image 3001/13690: Donald_Rumsfeld_0105.jpg\n",
      "\n",
      "Processing image 3101/13690: Eddie_Fenech_Adami_0001.jpg\n",
      "\n",
      "Processing image 3201/13690: Ekke_Hard_Forberg_0001.jpg\n",
      "\n",
      "Processing image 3301/13690: Emily_Robison_0001.jpg\n",
      "\n",
      "Processing image 3401/13690: Estella_Warren_0001.jpg\n",
      "\n",
      "Processing image 3501/13690: Fernando_Valenzuela_0001.jpg\n",
      "\n",
      "Processing image 3601/13690: Franz_Beckenbauer_0001.jpg\n",
      "\n",
      "Processing image 3701/13690: Gene_Robinson_0001.jpg\n",
      "\n",
      "Processing image 3801/13690: George_Ryan_0003.jpg\n",
      "\n",
      "Processing image 3901/13690: George_W_Bush_0096.jpg\n",
      "\n",
      "Processing image 4001/13690: George_W_Bush_0200.jpg\n",
      "\n",
      "Processing image 4101/13690: George_W_Bush_0302.jpg\n",
      "\n",
      "Processing image 4201/13690: George_W_Bush_0402.jpg\n",
      "\n",
      "Processing image 4301/13690: George_W_Bush_0504.jpg\n",
      "\n",
      "Processing image 4401/13690: Gerhard_Schroeder_0049.jpg\n",
      "\n",
      "Processing image 4501/13690: Gina_Lollobrigida_0001.jpg\n",
      "\n",
      "Processing image 4601/13690: Goran_Persson_0002.jpg\n",
      "\n",
      "Processing image 4701/13690: Guillaume_Cannet_0001.jpg\n",
      "\n",
      "Processing image 4801/13690: Hal_Gehman_0002.jpg\n",
      "\n",
      "Processing image 4901/13690: Harrison_Ford_0001.jpg\n",
      "\n",
      "Processing image 5001/13690: Henry_Suazo_0001.jpg\n",
      "\n",
      "Processing image 5101/13690: Howard_Dean_0009.jpg\n",
      "\n",
      "Processing image 5201/13690: Hugo_Conte_0001.jpg\n",
      "\n",
      "Processing image 5301/13690: Inam-ul-Haq_0002.jpg\n",
      "\n",
      "Processing image 5401/13690: Jack_Straw_0025.jpg\n",
      "\n",
      "Processing image 5501/13690: James_Blake_0008.jpg\n",
      "\n",
      "Processing image 5601/13690: Jamie_Dimon_0001.jpg\n",
      "\n",
      "Processing image 5701/13690: Javier_Solana_0004.jpg\n",
      "\n",
      "Processing image 5801/13690: Jean_Chretien_0007.jpg\n",
      "\n",
      "Processing image 5901/13690: Jennifer_Aniston_0001.jpg\n",
      "\n",
      "Processing image 6001/13690: Jennifer_Lopez_0019.jpg\n",
      "\n",
      "Processing image 6101/13690: Jessica_Brungo_0001.jpg\n",
      "\n",
      "Processing image 6201/13690: Jim_Tressel_0004.jpg\n",
      "\n",
      "Processing image 6301/13690: Joe_Strummer_0001.jpg\n",
      "\n",
      "Processing image 6401/13690: John_Bolton_0004.jpg\n",
      "\n",
      "Processing image 6501/13690: John_Malkovich_0002.jpg\n",
      "\n",
      "Processing image 6601/13690: John_Snow_0005.jpg\n",
      "\n",
      "Processing image 6701/13690: Jorge_Enrique_Jimenez_0001.jpg\n",
      "\n",
      "Processing image 6801/13690: Jose_Mourinho_0002.jpg\n",
      "\n",
      "Processing image 6901/13690: Judy_Spreckels_0001.jpg\n",
      "\n",
      "Processing image 7001/13690: Junichiro_Koizumi_0033.jpg\n",
      "\n",
      "Processing image 7101/13690: Kate_Hudson_0005.jpg\n",
      "\n",
      "Processing image 7201/13690: Kenenisa_Bekele_0001.jpg\n",
      "\n",
      "Processing image 7301/13690: Kim_Dong-hwa_0001.jpg\n",
      "\n",
      "Processing image 7401/13690: Krishna_Bhadur_Mahara_0001.jpg\n",
      "\n",
      "Processing image 7501/13690: Larry_Johnson_0001.jpg\n",
      "\n",
      "Processing image 7601/13690: Lawrence_Vito_0001.jpg\n",
      "\n",
      "Processing image 7701/13690: Leslie_Wiser_Jr_0001.jpg\n",
      "\n",
      "Processing image 7801/13690: Lisa_Stansfield_0001.jpg\n",
      "\n",
      "Processing image 7901/13690: Lucio_Gutierrez_0008.jpg\n",
      "\n",
      "Processing image 8001/13690: Lyle_Lovett_0001.jpg\n",
      "\n",
      "Processing image 8101/13690: Manuel_Jesus_0001.jpg\n",
      "\n",
      "Processing image 8201/13690: Maria_Wetterstrand_0001.jpg\n",
      "\n",
      "Processing image 8301/13690: Mark_Sisk_0001.jpg\n",
      "\n",
      "Processing image 8401/13690: Mary_Descenza_0001.jpg\n",
      "\n",
      "Processing image 8501/13690: Megawati_Sukarnoputri_0009.jpg\n",
      "\n",
      "Processing image 8601/13690: Michael_Boyce_0001.jpg\n",
      "\n",
      "Processing image 8701/13690: Michael_Richards_0001.jpg\n",
      "\n",
      "Processing image 8801/13690: Mike_Cunning_0001.jpg\n",
      "\n",
      "Processing image 8901/13690: Milo_Maestrecampo_0002.jpg\n",
      "\n",
      "Processing image 9001/13690: Mona_Rishmawi_0001.jpg\n",
      "\n",
      "Processing image 9101/13690: Nancy_Sinatra_0001.jpg\n",
      "\n",
      "Processing image 9201/13690: Nelson_Mandela_0004.jpg\n",
      "\n",
      "Processing image 9301/13690: Nicole_Kidman_0009.jpg\n",
      "\n",
      "Processing image 9401/13690: Omar_el-Heib_0001.jpg\n",
      "\n",
      "Processing image 9501/13690: Patricia_Clarkson_0003.jpg\n",
      "\n",
      "Processing image 9601/13690: Paul_Burrell_0006.jpg\n",
      "\n",
      "Processing image 9701/13690: Pa_Kou_Hang_0001.jpg\n",
      "\n",
      "Processing image 9801/13690: Peter_Ueberroth_0001.jpg\n",
      "\n",
      "Processing image 9901/13690: Polona_Bas_0001.jpg\n",
      "\n",
      "Processing image 10001/13690: Rachel_Griffiths_0003.jpg\n",
      "\n",
      "Processing image 10101/13690: Ray_Nagin_0002.jpg\n",
      "\n",
      "Processing image 10201/13690: Ricardo_Lagos_0010.jpg\n",
      "\n",
      "Processing image 10301/13690: Richard_Myers_0017.jpg\n",
      "\n",
      "Processing image 10401/13690: Robbie_Naish_0001.jpg\n",
      "\n",
      "Processing image 10501/13690: Robert_Redford_0008.jpg\n",
      "\n",
      "Processing image 10601/13690: Roger_Suarez_0001.jpg\n",
      "\n",
      "Processing image 10701/13690: Rosemarie_Stack_0002.jpg\n",
      "\n",
      "Processing image 10801/13690: Sabah_Al-Ahmad_Al-Jaber_Al-Sabah_0001.jpg\n",
      "\n",
      "Processing image 10901/13690: Sandra_Milo_0001.jpg\n",
      "\n",
      "Processing image 11001/13690: Sepp_Blatter_0001.jpg\n",
      "\n",
      "Processing image 11101/13690: Shane_Warne_0002.jpg\n",
      "\n",
      "Processing image 11201/13690: Silvio_Berlusconi_0012.jpg\n",
      "\n",
      "Processing image 11301/13690: Stefaan_Declerk_0001.jpg\n",
      "\n",
      "Processing image 11401/13690: Steve_Nash_0001.jpg\n",
      "\n",
      "Processing image 11501/13690: Tab_Turner_0001.jpg\n",
      "\n",
      "Processing image 11601/13690: Terrell_Suggs_0001.jpg\n",
      "\n",
      "Processing image 11701/13690: Tiger_Woods_0012.jpg\n",
      "\n",
      "Processing image 11801/13690: Tommy_Franks_0006.jpg\n",
      "\n",
      "Processing image 11901/13690: Tom_Hanks_0009.jpg\n",
      "\n",
      "Processing image 12001/13690: Tony_Blair_0023.jpg\n",
      "\n",
      "Processing image 12101/13690: Tony_Blair_0124.jpg\n",
      "\n",
      "Processing image 12201/13690: Tyler_Hamilton_0002.jpg\n",
      "\n",
      "Processing image 12301/13690: Vicente_Fox_0012.jpg\n",
      "\n",
      "Processing image 12401/13690: Vladimir_Putin_0026.jpg\n",
      "\n",
      "Processing image 12501/13690: Wesley_Clark_0001.jpg\n",
      "\n",
      "Processing image 12601/13690: Win_Aung_0004.jpg\n",
      "\n",
      "Processing image 12701/13690: Yossi_Beilin_0001.jpg\n",
      "\n",
      "Processing image 12801/13690: Dileep_10.jpg\n",
      "  Skipping image with incorrect shape: (210, 189)\n",
      "\n",
      "Processing image 12901/13690: Dwarakish_10.jpg\n",
      "  Skipping image with incorrect shape: (141, 103)\n",
      "\n",
      "Processing image 13001/13690: Dwarakish_21.jpg\n",
      "  Skipping image with incorrect shape: (135, 106)\n",
      "\n",
      "Processing image 13101/13690: FaridaJalal_3.jpg\n",
      "  Skipping image with incorrect shape: (102, 89)\n",
      "\n",
      "Processing image 13201/13690: HrithikRoshan_121.jpg\n",
      "  Skipping image with incorrect shape: (39, 31)\n",
      "\n",
      "Processing image 13301/13690: HrithikRoshan_222.jpg\n",
      "  Skipping image with incorrect shape: (149, 104)\n",
      "\n",
      "Processing image 13401/13690: Jayamadhuri_127.jpg\n",
      "  Skipping image with incorrect shape: (68, 38)\n",
      "\n",
      "Processing image 13501/13690: Jayamadhuri_44.jpg\n",
      "  Skipping image with incorrect shape: (55, 42)\n",
      "\n",
      "Processing image 13601/13690: KatrinaKaif_72.jpg\n",
      "  Skipping image with incorrect shape: (213, 104)\n",
      "\n",
      "Processing summary:\n",
      "  Successfully processed: 12765 images\n",
      "  Errors/skipped: 925 images\n",
      "(10212, 2500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 42\n",
    "images, labels, classes, filepaths = convert_data()\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(images, labels, test_size=0.2, random_state=seed)\n",
    "print(Xtrain.shape)\n",
    "print(Xval.shape)\n",
    "num_classes = len(Ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[99201024,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhaustedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m model.add(layers.Conv2D(\u001b[32m64\u001b[39m, (\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m), activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m model.add(layers.Flatten())\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m model.add(layers.Dropout(\u001b[32m0.2\u001b[39m,seed=seed))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# model.add(layers.Dense(64, activation='relu'))\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# model.add(layers.Dropout(0.2,seed=seed*2))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:122\u001b[39m, in \u001b[36mSequential.add\u001b[39m\u001b[34m(self, layer, rebuild)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mself\u001b[39m._layers.append(layer)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.built = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:149\u001b[39m, in \u001b[36mSequential._maybe_rebuild\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    148\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].batch_shape\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[32m    154\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].input_shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:229\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    228\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    231\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py:195\u001b[39m, in \u001b[36mSequential.build\u001b[39m\u001b[34m(self, input_shape)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m1\u001b[39m:]:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\brand\\OneDrive\\Desktop\\CS178\\final_project\\cs178_facial_expression\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\random.py:34\u001b[39m, in \u001b[36muniform\u001b[39m\u001b[34m(shape, minval, maxval, dtype, seed)\u001b[39m\n\u001b[32m     32\u001b[39m dtype = dtype \u001b[38;5;129;01mor\u001b[39;00m floatx()\n\u001b[32m     33\u001b[39m seed = _cast_seed(draw_seed(seed))\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mResourceExhaustedError\u001b[39m: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[99201024,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:Mul] name: "
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(10000, 2500, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.2,seed=seed))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dropout(0.2,seed=seed*2))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# build the CNN model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the CNN model\n",
    "history = model.fit(Xtrain, Ytrain, epochs=3, validation_data=(Xval, Yval))\n",
    "\n",
    "# save the CNN model to saved_networks/tfmodel.keras\n",
    "# model.save(save_path)\n",
    "\n",
    "# evalulate the CNN model\n",
    "result = model.evaluate(Xval, Yval)\n",
    "result_dict = dict(zip(model.metrics_names, result))\n",
    "print(f\"Model Result\\n-----\\n{result_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
